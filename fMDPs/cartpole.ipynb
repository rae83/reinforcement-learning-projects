{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cartpole.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"UCQNkqT7b9Wa","colab_type":"text"},"cell_type":"markdown","source":["# Cartpole"]},{"metadata":{"id":"aChQupelafLN","colab_type":"text"},"cell_type":"markdown","source":["### Steps\n","\n","For Deep Q Agent, define:\n","- the DNN model\n","- action selection procedure\n","  - will need epsilon and epsilon decay parameters\n","- memory and replay functions for learning\n","  - will need learning rate and batch size parameters\n","  - minimize cost (MSE) where target value is current reward + discounted (by factor of gamma) predicted reward of next state\n","\n","Uses https://keon.io/deep-q-learning/ as reference\n"]},{"metadata":{"id":"cq5KsX9scN-8","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":14}],"base_uri":"https://localhost:8080/","height":510},"outputId":"2a0442d7-5e31-4e95-d3db-97e8fbfe39c5","executionInfo":{"status":"ok","timestamp":1518642612854,"user_tz":300,"elapsed":7717,"user":{"displayName":"Ryan Enderby","photoUrl":"//lh5.googleusercontent.com/-XrtHXtwuriM/AAAAAAAAAAI/AAAAAAAAAOE/Ggd8rX7UVek/s50-c-k-no/photo.jpg","userId":"105882398870558104706"}}},"cell_type":"code","source":["!pip install gym\n","!pip install keras"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting gym\n","  Downloading gym-0.9.7.tar.gz (108kB)\n","\u001b[K    100% |████████████████████████████████| 112kB 2.3MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym)\n","Collecting pyglet>=1.2.0 (from gym)\n","  Downloading pyglet-1.3.1-py2.py3-none-any.whl (1.0MB)\n","\u001b[K    100% |████████████████████████████████| 1.0MB 1.1MB/s \n","\u001b[?25hRequirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym)\n","Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym)\n","Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym)\n","Building wheels for collected packages: gym\n","  Running setup.py bdist_wheel for gym ... \u001b[?25l-\b \b\\\b \bdone\n","\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/a8/e4/fc/145832d732d33de702076907d7c3b4c47ba4302dbedd35fc80\n","Successfully built gym\n","Installing collected packages: pyglet, gym\n","Successfully installed gym-0.9.7 pyglet-1.3.1\n","Collecting keras\n","  Downloading Keras-2.1.4-py2.py3-none-any.whl (322kB)\n","\u001b[K    100% |████████████████████████████████| 327kB 2.1MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras)\n","Installing collected packages: keras\n","Successfully installed keras-2.1.4\n"],"name":"stdout"}]},{"metadata":{"id":"4IZwdmCR7F3n","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":6}],"base_uri":"https://localhost:8080/","height":275},"outputId":"5cb4b60b-9653-4c41-b423-808fdb7085aa","executionInfo":{"status":"ok","timestamp":1518642598451,"user_tz":300,"elapsed":23159,"user":{"displayName":"Ryan Enderby","photoUrl":"//lh5.googleusercontent.com/-XrtHXtwuriM/AAAAAAAAAAI/AAAAAAAAAOE/Ggd8rX7UVek/s50-c-k-no/photo.jpg","userId":"105882398870558104706"}}},"cell_type":"code","source":["# Code in this cell from: https://www.kaggle.com/getting-started/47096 -- mount directory so that I can read / write files from drive\n","\n","# Install a Drive FUSE wrapper.\n","# https://github.com/astrada/google-drive-ocamlfuse\n","!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","!apt-get update -qq 2>&1 > /dev/null\n","!apt-get -y install -qq google-drive-ocamlfuse fuse\n","\n","\n","\n","# Generate auth tokens for Colab\n","from google.colab import auth\n","auth.authenticate_user()\n","\n","\n","# Generate creds for the Drive FUSE library.\n","from oauth2client.client import GoogleCredentials\n","creds = GoogleCredentials.get_application_default()\n","import getpass\n","!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","vcode = getpass.getpass()\n","!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n","\n","\n","# Create a directory and mount Google Drive using that directory.\n","!mkdir -p drive\n","!google-drive-ocamlfuse drive\n","\n","print('Files in Drive:')\n","!ls drive/\n","\n","# Create a file in Drive.\n","!echo \"This newly created file will appear in your Drive file list.\" > drive/created.txt"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\r\n","··········\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","Please enter the verification code: Access token retrieved correctly.\n","Files in Drive:\n","Australia Itinerary.ods  Fall 2015\t\tPictures-for-craigslist\n","CCC\t\t\t Ireland itinerary.ods\tResumes\n","colab_notebooks\t\t LPs\t\t\tSarangi\n","Colab Notebooks\t\t Misc-personal\t\tsaved_models\n","Columbia\t\t MPS Fall 2016\t\tSensoDx\n","Columbia-backup\t\t MPS Spring 2016\tSH\n","created.txt\t\t Old schoolwork\t\tSt. Croix expenses!.ods\n","c_y_w_mp3s\t\t Packing list.ods\tVideos\n","DX\t\t\t Pictures\n"],"name":"stdout"}]},{"metadata":{"id":"Mp_SHtDfb4qz","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":34},"outputId":"837e5991-c681-4118-f104-624ae392eddc","executionInfo":{"status":"ok","timestamp":1518642620087,"user_tz":300,"elapsed":7215,"user":{"displayName":"Ryan Enderby","photoUrl":"//lh5.googleusercontent.com/-XrtHXtwuriM/AAAAAAAAAAI/AAAAAAAAAOE/Ggd8rX7UVek/s50-c-k-no/photo.jpg","userId":"105882398870558104706"}}},"cell_type":"code","source":["import gym\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import random\n","from collections import deque\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.optimizers import Adam"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"3ByGVB9Cb4q-","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["class DeepQAgent(object):\n","  \n","  \n","  def __init__(self, state_size, action_size):\n","    self.state_size = state_size\n","    self.action_size = action_size\n","    \n","    self.learning_rate = 0.001\n","    self.gamma = 0.1\n","\n","    self.epsilon = 0.01 # set low if using pre-trained weights\n","    self.epsilon_decay = 0.005\n","    self.min_epsilon = 0.01\n","    \n","    self.replay_buffer = deque(maxlen=10000) # from stackoverflow: https://stackoverflow.com/questions/23487307/python-deque-vs-list-performance-comparison\n","    self.model = self.build_model()\n","\n","    \n","  def build_model(self):\n","    \n","    # Construct a NN with two hidden layers, rectified linear unit activation\n","    model = Sequential()\n","    model.add(Dense(24, input_dim=self.state_size, activation='relu')) # input layer size (state_size,)\n","    model.add(Dense(24, activation='relu'))\n","    model.add(Dense(self.action_size, activation='linear')) # output layer size (action_size,). Linear because we want to map to Q values, e.g. not softmax probabilities\n","    \n","    model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n","    \n","    return model\n","\n","  \n","  def choose_action(self, state):\n","    if (np.random.rand() <= self.epsilon): # explore with probability of epsilon\n","      return env.action_space.sample()\n","    else:\n","      q_values = self.model.predict(state)\n","      return np.argmax(q_values[0]) # choose the action that maximizes q value\n","\n","    \n","  def add_to_replay_buffer(self, state, action, reward, next_state, done):\n","    self.replay_buffer.append((state, action, reward, next_state, done))\n","\n","    \n","  def replay(self, batch_size):\n","    # sample a batch from memory\n","    batch = random.sample(self.replay_buffer, batch_size)\n","\n","    # for the sampled data in batch\n","    for state, action, reward, next_state, done in batch:\n","      target = reward\n","\n","      if not done: # if the state isnt terminal, predict the future discounted reward.  Else, target value is equal to the reward received\n","        target = reward + (1 - self.gamma) * np.amax(self.model.predict(next_state)[0]) # return the highest VALUE (not the action), since we are updating target reward\n","\n","      # train the agent to map current state to future (discounted) reward\n","      q_values = self.model.predict(state) # predict the values (since we don't know what both actions would have yielded)\n","      q_values[0][action] = target # and then reassign the taken action value to the reward / target\n","\n","      self.model.fit(state, q_values, epochs=1, verbose=0)\n","\n","    if self.epsilon > self.min_epsilon:\n","      self.epsilon *= (1 - self.epsilon_decay)\n","\n","      \n","  def load_weights(self, name):\n","    self.model.load_weights(name)\n","\n","    \n","  def save_weights(self, name):\n","    self.model.save_weights(name)\n","        "],"execution_count":0,"outputs":[]},{"metadata":{"id":"HZqHGMLyIxPC","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# helper function to transpose returned state so that it can serve as input to model\n","\n","def transpose(state, dimension):\n","  return np.reshape(state, [1, dimension])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"taH10oPIb4q3","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":34},"outputId":"e6a744a8-7525-48f2-dc80-b5004fa68ae9","executionInfo":{"status":"ok","timestamp":1518642730175,"user_tz":300,"elapsed":254,"user":{"displayName":"Ryan Enderby","photoUrl":"//lh5.googleusercontent.com/-XrtHXtwuriM/AAAAAAAAAAI/AAAAAAAAAOE/Ggd8rX7UVek/s50-c-k-no/photo.jpg","userId":"105882398870558104706"}}},"cell_type":"code","source":["env = gym.make(\"CartPole-v0\")\n","observation = env.reset()\n","\n","# print(vars(env.action_space))\n","# print(vars(env.observation_space))\n","\n","action_size = env.action_space.n\n","state_size = env.observation_space.shape[0]"],"execution_count":12,"outputs":[{"output_type":"stream","text":["\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"],"name":"stdout"}]},{"metadata":{"id":"Dh7nfLvub4rC","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":6},{"item_id":7}],"base_uri":"https://localhost:8080/","height":1575},"outputId":"34892d10-d22f-444f-de5f-b143ec0ac968","executionInfo":{"status":"error","timestamp":1518642756055,"user_tz":300,"elapsed":25603,"user":{"displayName":"Ryan Enderby","photoUrl":"//lh5.googleusercontent.com/-XrtHXtwuriM/AAAAAAAAAAI/AAAAAAAAAOE/Ggd8rX7UVek/s50-c-k-no/photo.jpg","userId":"105882398870558104706"}}},"cell_type":"code","source":["EPISODES = 1000\n","MAX_STEPS = 500 # because we don't want this to go on forever (should the model ever get that good ;) )\n","SOLVED_STEPS = 195 # openAI says that solving is averaging at least 195 steps per trial\n","BATCH_SIZE = 32\n","\n","FILE_NAME = 'drive/saved_models/cartpole_dqn_weights.h5'\n","\n","\n","agent = DeepQAgent(state_size, action_size)\n","agent.load_weights(FILE_NAME)\n","\n","timesteps = 0\n","max_timesteps_so_far = 0\n","\n","for i in range(EPISODES):\n","  \n","  state = env.reset()\n","  state = transpose(state, state_size)\n","  \n","  for t in range(MAX_STEPS):\n","    # env.render() # can use this to watch visualization on a VM that supports it\n","    \n","    action = agent.choose_action(state)\n","    next_state, reward, done, _ = env.step(action)\n","    next_state = transpose(next_state, state_size)\n","    agent.add_to_replay_buffer(state, action, reward, next_state, done)\n","    \n","    state = next_state\n","\n","    if done:\n","      # calculate average timesteps over episode_range number of episodes\n","      timesteps += t\n","      episode_range = 10\n","      \n","      if i != 0 and i % episode_range == 0:\n","        print(\"Episodes {} through {} finished after an average of {} timesteps\".format(i - episode_range, i, timesteps / episode_range))\n","        if timesteps / episode_range > max_timesteps_so_far:\n","          max_timesteps_so_far = timesteps / episode_range\n","#           agent.save_weights(FILE_NAME)\n","#           print('Model weights saved.')\n","\n","        if timesteps / episode_range >= SOLVED_STEPS:\n","          print('Episodes were a sucess!')\n","    \n","        timesteps = 0\n","      break\n","      \n","  if len(agent.replay_buffer) > BATCH_SIZE: # start learning once we have seen enough examples to form a batch to train on\n","    agent.replay(BATCH_SIZE)\n","\n","# env.render()\n","env.reset()\n","env.close()"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Episodes 0 through 10 finished after an average of 215.8 timesteps\n","Episodes were a sucess!\n","Episodes 10 through 20 finished after an average of 190.5 timesteps\n","Episodes 20 through 30 finished after an average of 199.0 timesteps\n","Episodes were a sucess!\n","Episodes 30 through 40 finished after an average of 193.9 timesteps\n","Episodes 40 through 50 finished after an average of 194.0 timesteps\n","Episodes 50 through 60 finished after an average of 199.0 timesteps\n","Episodes were a sucess!\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-9d5d9f10a748>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# env.render() # can use this to watch visualization on a VM that supports it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-2573934b0ebc>\u001b[0m in \u001b[0;36mchoose_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     34\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m       \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# choose the action that maximizes q value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m         return self.model.predict(x, batch_size=batch_size, verbose=verbose,\n\u001b[0;32m-> 1025\u001b[0;31m                                   steps=steps)\n\u001b[0m\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1840\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m         return self._predict_loop(f, ins, batch_size=batch_size,\n\u001b[0;32m-> 1842\u001b[0;31m                                   verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1844\u001b[0m     def train_on_batch(self, x, y,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1335\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1337\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1338\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m                     \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"id":"EwIT0YsipILD","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}