{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UCQNkqT7b9Wa"
   },
   "source": [
    "# Cartpole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aChQupelafLN"
   },
   "source": [
    "### Steps\n",
    "\n",
    "For Deep Q Agent, define:\n",
    "- the DNN model\n",
    "- action selection procedure\n",
    "  - will need epsilon and epsilon decay parameters\n",
    "- memory and replay functions for learning\n",
    "  - will need learning rate and batch size parameters\n",
    "  - minimize cost (MSE) where target value is current reward + discounted (by factor of gamma) predicted reward of next state\n",
    "\n",
    "Uses https://keon.io/deep-q-learning/ as reference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7215,
     "status": "ok",
     "timestamp": 1518642620087,
     "user": {
      "displayName": "Ryan Enderby",
      "photoUrl": "//lh5.googleusercontent.com/-XrtHXtwuriM/AAAAAAAAAAI/AAAAAAAAAOE/Ggd8rX7UVek/s50-c-k-no/photo.jpg",
      "userId": "105882398870558104706"
     },
     "user_tz": 300
    },
    "id": "Mp_SHtDfb4qz",
    "outputId": "837e5991-c681-4118-f104-624ae392eddc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "3ByGVB9Cb4q-"
   },
   "outputs": [],
   "source": [
    "class DeepQAgent(object):\n",
    "  \n",
    "  \n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.learning_rate = 0.001\n",
    "        self.gamma = 0.1\n",
    "\n",
    "        self.epsilon = 0.01 # set low if using pre-trained weights\n",
    "        self.epsilon_decay = 0.005\n",
    "        self.min_epsilon = 0.01\n",
    "\n",
    "        self.replay_buffer = deque(maxlen=10000) # from stackoverflow: https://stackoverflow.com/questions/23487307/python-deque-vs-list-performance-comparison\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    \n",
    "    def build_model(self):\n",
    "    \n",
    "        # Construct a NN with two hidden layers, rectified linear unit activation\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu')) # input layer size (state_size,)\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear')) # output layer size (action_size,). Linear because we want to map to Q values, e.g. not softmax probabilities\n",
    "\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "\n",
    "        return model\n",
    "\n",
    "  \n",
    "    def choose_action(self, state):\n",
    "        if (np.random.rand() <= self.epsilon): # explore with probability of epsilon\n",
    "            return env.action_space.sample()\n",
    "        else:\n",
    "            q_values = self.model.predict(state)\n",
    "            return np.argmax(q_values[0]) # choose the action that maximizes q value\n",
    "\n",
    "    \n",
    "    def add_to_replay_buffer(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        # sample a batch from memory\n",
    "        batch = random.sample(self.replay_buffer, batch_size)\n",
    "\n",
    "        # for the sampled data in batch\n",
    "        for state, action, reward, next_state, done in batch:\n",
    "            target = reward\n",
    "\n",
    "            if not done: # if the state isnt terminal, predict the future discounted reward.  Else, target value is equal to the reward received\n",
    "                target = reward + (1 - self.gamma) * np.amax(self.model.predict(next_state)[0]) # return the highest VALUE (not the action), since we are updating target reward\n",
    "\n",
    "            # train the agent to map current state to future (discounted) reward\n",
    "            q_values = self.model.predict(state) # predict the values (since we don't know what both actions would have yielded)\n",
    "            q_values[0][action] = target # and then reassign the taken action value to the reward / target\n",
    "\n",
    "            self.model.fit(state, q_values, epochs=1, verbose=0)\n",
    "\n",
    "        if self.epsilon > self.min_epsilon:\n",
    "            self.epsilon *= (1 - self.epsilon_decay)\n",
    "\n",
    "      \n",
    "    def load_weights(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    \n",
    "    def save_weights(self, name):\n",
    "        self.model.save_weights(name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "HZqHGMLyIxPC"
   },
   "outputs": [],
   "source": [
    "# helper function to transpose returned state so that it can serve as input to model\n",
    "\n",
    "def transpose(state, dimension):\n",
    "    return np.reshape(state, [1, dimension])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 254,
     "status": "ok",
     "timestamp": 1518642730175,
     "user": {
      "displayName": "Ryan Enderby",
      "photoUrl": "//lh5.googleusercontent.com/-XrtHXtwuriM/AAAAAAAAAAI/AAAAAAAAAOE/Ggd8rX7UVek/s50-c-k-no/photo.jpg",
      "userId": "105882398870558104706"
     },
     "user_tz": 300
    },
    "id": "taH10oPIb4q3",
    "outputId": "e6a744a8-7525-48f2-dc80-b5004fa68ae9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-02-18 18:16:54,489] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "observation = env.reset()\n",
    "\n",
    "# print(vars(env.action_space))\n",
    "# print(vars(env.observation_space))\n",
    "\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1575,
     "output_extras": [
      {
       "item_id": 6
      },
      {
       "item_id": 7
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 25603,
     "status": "error",
     "timestamp": 1518642756055,
     "user": {
      "displayName": "Ryan Enderby",
      "photoUrl": "//lh5.googleusercontent.com/-XrtHXtwuriM/AAAAAAAAAAI/AAAAAAAAAOE/Ggd8rX7UVek/s50-c-k-no/photo.jpg",
      "userId": "105882398870558104706"
     },
     "user_tz": 300
    },
    "id": "Dh7nfLvub4rC",
    "outputId": "34892d10-d22f-444f-de5f-b143ec0ac968"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodes 0 through 10 finished after an average of 211.2 timesteps\n",
      "Episodes were a sucess!\n",
      "Episodes 10 through 20 finished after an average of 194.5 timesteps\n",
      "Episodes 20 through 30 finished after an average of 193.1 timesteps\n",
      "Episodes 30 through 40 finished after an average of 194.2 timesteps\n",
      "Episodes 40 through 50 finished after an average of 189.5 timesteps\n",
      "Episodes 50 through 60 finished after an average of 194.1 timesteps\n",
      "Episodes 60 through 70 finished after an average of 197.5 timesteps\n",
      "Episodes were a sucess!\n",
      "Episodes 70 through 80 finished after an average of 196.8 timesteps\n",
      "Episodes were a sucess!\n",
      "Episodes 80 through 90 finished after an average of 192.3 timesteps\n"
     ]
    }
   ],
   "source": [
    "EPISODES = 100\n",
    "MAX_STEPS = 500 # because we don't want this to go on forever (should the model ever get that good ;) )\n",
    "SOLVED_STEPS = 195 # openAI says that solving is averaging at least 195 steps per trial\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "FILE_NAME = 'saved_models/cartpole_dqn_weights.h5'\n",
    "\n",
    "\n",
    "agent = DeepQAgent(state_size, action_size)\n",
    "agent.load_weights(FILE_NAME)\n",
    "\n",
    "timesteps = 0\n",
    "max_timesteps_so_far = 0\n",
    "\n",
    "for i in range(EPISODES):\n",
    "  \n",
    "    state = env.reset()\n",
    "    state = transpose(state, state_size)\n",
    "\n",
    "    for t in range(MAX_STEPS):\n",
    "        # env.render() # can use this to watch visualization on a VM that supports it\n",
    "\n",
    "        action = agent.choose_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = transpose(next_state, state_size)\n",
    "        agent.add_to_replay_buffer(state, action, reward, next_state, done)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            # calculate average timesteps over episode_range number of episodes\n",
    "            timesteps += t\n",
    "            episode_range = 10\n",
    "\n",
    "            if i != 0 and i % episode_range == 0:\n",
    "                print(\"Episodes {} through {} finished after an average of {} timesteps\".format(i - episode_range, i, timesteps / episode_range))\n",
    "                if timesteps / episode_range > max_timesteps_so_far:\n",
    "                    max_timesteps_so_far = timesteps / episode_range\n",
    "        #           agent.save_weights(FILE_NAME)\n",
    "        #           print('Model weights saved.')\n",
    "\n",
    "                if timesteps / episode_range >= SOLVED_STEPS:\n",
    "                    print('Episodes were a sucess!')\n",
    "\n",
    "                timesteps = 0\n",
    "            break\n",
    "\n",
    "    if len(agent.replay_buffer) > BATCH_SIZE: # start learning once we have seen enough examples to form a batch to train on\n",
    "        agent.replay(BATCH_SIZE)\n",
    "\n",
    "# env.render()\n",
    "env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "EwIT0YsipILD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "cartpole.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
